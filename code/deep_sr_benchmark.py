"""Trains the deep symbolic regression architecture on given functions to produce a simple equation that describes
the dataset. Uses L_0 regularization for the EQL network."""

import pickle
from matplotlib import pyplot as plt
import numpy as np
import os
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from utils import pretty_print, functions
from Training.symbolic_network import SymbolicNetL0
from inspect import signature
import time
import argparse
from Metric.RobustnessMetric import RobustnessMetric
N_TRAIN = 256       # Size of training dataset
N_VAL = 100         # Size of validation dataset
DOMAIN = (-1, 1)    # Domain of dataset - range from which we sample x
# DOMAIN = np.array([[0, -1, -1], [1, 1, 1]])   # Use this format if each input variable has a different domain
N_TEST = 100        # Size of test dataset
DOMAIN_TEST = (-0.5, 0.5)   # Domain of test dataset - should be larger than training domain to test extrapolation
NOISE_SD = 0        # Standard deviation of noise for training dataset
var_names = ["x", "y", "z"]

# Standard deviation of random distribution for weight initializations.
init_sd_first = 0.1
init_sd_last = 1.0
init_sd_middle = 0.5
# init_sd_first = 0.5
# init_sd_last = 0.5
# init_sd_middle = 0.5
# init_sd_first = 0.1
# init_sd_last = 0.1
# init_sd_middle = 0.1


def generate_data(func, N, range_min=DOMAIN[0], range_max=DOMAIN[1]):
    """Generates datasets."""
    x_dim = len(signature(func).parameters)     # Number of inputs to the function, or, dimensionality of x
    # torch.manual_seed(0) # for reproducibility; seed can be changed in case we want different datasets
    x = (range_max - range_min) * torch.rand([N, x_dim]) + range_min
    y = torch.tensor([[func(*x_i)] for x_i in x])
    return x, y


class Benchmark:
    """Benchmark object just holds the results directory (results_dir) to save to and the hyper-parameters. So it is
    assumed all the results in results_dir share the same hyper-parameters. This is useful for benchmarking multiple
    functions with the same hyper-parameters."""
    def __init__(self, results_dir, n_layers=2, reg_weight=5e-3, learning_rate=1e-2,
                 n_epochs1=5001, n_epochs2=5001):
        """Set hyper-parameters"""
        self.activation_funcs = [
            *[functions.Constant()] * 2,
            *[functions.Identity()] * 4,
            *[functions.Square()] * 4,
            *[functions.Sin()] * 2,
            *[functions.Exp()] * 2,
            *[functions.Sigmoid()] * 2,
            *[functions.Product(1.0)] * 2
        ]

        self.n_layers = n_layers                # Number of hidden layers
        self.reg_weight = reg_weight            # Regularization weight
        self.learning_rate = learning_rate
        self.summary_step = 1000                # Number of iterations at which to print to screen
        self.n_epochs1 = n_epochs1
        self.n_epochs2 = n_epochs2

        if not os.path.exists(results_dir):
            os.makedirs(results_dir)
        self.results_dir = results_dir

        # Save hyperparameters to file
        result = {
            "learning_rate": self.learning_rate,
            "summary_step": self.summary_step,
            "n_epochs1": self.n_epochs1,
            "n_epochs2": self.n_epochs2,
            "activation_funcs_name": [func.name for func in self.activation_funcs],
            "n_layers": self.n_layers,
            "reg_weight": self.reg_weight,
        }
        with open(os.path.join(self.results_dir, 'params.pickle'), "wb+") as f:
            pickle.dump(result, f)

    def benchmark(self, func, func_name, trials):
        """Benchmark the EQL network on data generated by the given function. Print the results ordered by test error.

        Arguments:
            func: lambda function to generate dataset
            func_name: string that describes the function - this will be the directory name
            trials: number of trials to train from scratch. Will save the results for each trial.
        """

        print("Starting benchmark for function:\t%s" % func_name)
        print("==============================================")

        # Create a new sub-directory just for the specific function
        func_dir = os.path.join(self.results_dir, func_name)
        if not os.path.exists(func_dir):
            os.makedirs(func_dir)

        # Train network!
        expr_list, error_test_list = self.train(func, func_name, trials, func_dir)

        # Sort the results by test error (increasing) and print them to file
        # This allows us to easily count how many times it fit correctly.
        error_expr_sorted = sorted(zip(error_test_list, expr_list))     # List of (error, expr)
        error_test_sorted = [x for x, _ in error_expr_sorted]   # Separating out the errors
        expr_list_sorted = [x for _, x in error_expr_sorted]    # Separating out the expr

        fi = open(os.path.join(self.results_dir, 'eq_summary.txt'), 'a')
        fi.write("\n{}\n".format(func_name))
        for i in range(trials):
            fi.write("[%f]\t\t%s\n" % (error_test_sorted[i], str(expr_list_sorted[i])))
        fi.close()

    def train(self, func, func_name='', trials=1, func_dir='results/test'):
        """Train the network to find a given function"""

        use_cuda = torch.cuda.is_available()
        device = torch.device("cuda" if use_cuda else "cpu")
        print("Use cuda:", use_cuda, "Device:", device)

        x, y = generate_data(func, N_TRAIN)
        data, target = x.to(device), y.to(device)
        # x_val, y_val = generate_data(func, N_VAL)
        x_test, y_test = generate_data(func, N_TEST, range_min=DOMAIN_TEST[0], range_max=DOMAIN_TEST[1])
        test_data, test_target = x_test.to(device), y_test.to(device)

        # Setting up the symbolic regression network
        x_dim = len(signature(func).parameters)  # Number of input arguments to the function

        width = len(self.activation_funcs)
        n_double = functions.count_double(self.activation_funcs)

        # Arrays to keep track of various quantities as a function of epoch
        loss_list = []          # Total loss (MSE + regularization)
        error_list = []         # MSE
        reg_list = []           # Regularization
        error_test_list = []    # Test error

        error_test_final = []
        eq_list = []

        for trial in range(trials):
            print("Training on function " + func_name + " Trial " + str(trial+1) + " out of " + str(trials))

            # reinitialize for each trial
            net = SymbolicNetL0(self.n_layers,
                                funcs=self.activation_funcs,
                                initial_weights=[
                                    # kind of a hack for truncated normal
                                    torch.fmod(torch.normal(0, init_sd_first, size=(x_dim, width + n_double)), 2),
                                    torch.fmod(torch.normal(0, init_sd_middle, size=(width, width + n_double)), 2),
                                    torch.fmod(torch.normal(0, init_sd_middle, size=(width, width + n_double)), 2),
                                    torch.fmod(torch.normal(0, init_sd_last, size=(width, 1)), 2)
                                ]).to(device)

            loss_val = np.nan
            while np.isnan(loss_val):
                # training restarts if gradients blow up
                criterion = nn.MSELoss()
                optimizer = optim.RMSprop(net.parameters(),
                                          lr=self.learning_rate * 10,
                                          alpha=0.9,  # smoothing constant
                                          eps=1e-10,
                                          momentum=0.0,
                                          centered=False)

                # adapative learning rate
                lmbda = lambda epoch: 0.1
                scheduler = optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)
                # for param_group in optimizer.param_groups:
                #     print("Learning rate: %f" % param_group['lr'])

                t0 = time.time()

                # 0th warmup stage, then 2 stages of training with decreasing learning rate
                for epoch in range(self.n_epochs1 + self.n_epochs2 + 2000):
                    optimizer.zero_grad()  # zero the parameter gradients
                    outputs = net(data)  # forward pass
                    mse_loss = criterion(outputs, target)

                    reg_loss = net.get_loss()
                    loss = mse_loss + self.reg_weight * reg_loss
                    # TO-DO: add the penalization term based on the robustness
                    # loss = loss + 1 
                    loss.backward()
                    optimizer.step()

                    if epoch % self.summary_step == 0:
                        error_val = mse_loss.item()
                        reg_val = reg_loss.item()
                        loss_val = loss.item()
                        error_list.append(error_val)
                        reg_list.append(reg_val)
                        loss_list.append(loss_val)

                        with torch.no_grad():  # test error
                            test_outputs = net(test_data)
                            test_loss = F.mse_loss(test_outputs, test_target)
                            error_test_val = test_loss.item()
                            error_test_list.append(error_test_val)

                        print("Epoch: %d\tTotal training loss: %f\tTest error: %f" % (epoch, loss_val, error_test_val))

                        if np.isnan(loss_val):  # If loss goes to NaN, restart training
                            break

                    if epoch == 2000:
                        scheduler.step()  # lr /= 10
                    elif epoch == self.n_epochs1 + 2000:
                        scheduler.step()    # lr /= 10 again

                scheduler.step()  # lr /= 10 again

                t1 = time.time()

            tot_time = t1 - t0
            print(tot_time)

            # Print the expressions
            with torch.no_grad():
                weights = net.get_weights()
                expr = pretty_print.network(weights, self.activation_funcs, var_names[:x_dim])
                print(expr)

            # Save results
            trial_file = os.path.join(func_dir, 'trial%d.pickle' % trial)
            results = {
                "weights": weights,
                "loss_list": loss_list,
                "error_list": error_list,
                "reg_list": reg_list,
                "error_test": error_test_list,
                "expr": expr,
                "runtime": tot_time
            }
            with open(trial_file, "wb+") as f:
                pickle.dump(results, f)

            error_test_final.append(error_test_list[-1])
            eq_list.append(expr)

        return eq_list, error_test_final

    def load_and_test(self, x, y, trial_file):
        """Load a saved model from a pickle file and test it on new data."""
        use_cuda = torch.cuda.is_available()
        device = torch.device("cuda" if use_cuda else "cpu")
        print("Use cuda:", use_cuda, "Device:", device)

        # Load the saved model
        with open(trial_file, "rb") as f:
            results = pickle.load(f)

        weights = results["weights"]
        expr = results["expr"]
        print("Loaded model expression:", expr)
        x_test, y_test = x.to(device), y.to(device)
        # Generate test data
        test_data, test_target = x_test.to(device), y_test.to(device)

        # Setting up the symbolic regression network
        x_dim = len(signature(func).parameters)  # Number of input arguments to the function
        width = len(self.activation_funcs)
        n_double = functions.count_double(self.activation_funcs)

        initial_weights = [torch.tensor(w).to(device) for w in weights]

        net = SymbolicNetL0(self.n_layers,
                            funcs=self.activation_funcs,
                            initial_weights=initial_weights).to(device)

        # Test the network
        with torch.no_grad():
            test_outputs = net(test_data)
            test_loss = F.mse_loss(test_outputs, test_target)
            error_test_val = test_loss.item()
            print("Test error:", error_test_val)

        return net, error_test_val

    def load_and_test_func(self, func, trial_file):
            """Load a saved model from a pickle file and test it on new data."""
            use_cuda = torch.cuda.is_available()
            device = torch.device("cuda" if use_cuda else "cpu")
            print("Use cuda:", use_cuda, "Device:", device)

            # Load the saved model
            with open(trial_file, "rb") as f:
                results = pickle.load(f)

            weights = results["weights"]
            expr = results["expr"]
            print("Loaded model expression:", expr)

            # Generate test data
            x_test, y_test = generate_data(func, N_TEST, range_min=DOMAIN_TEST[0], range_max=DOMAIN_TEST[1])
            test_data, test_target = x_test.to(device), y_test.to(device)

            # Setting up the symbolic regression network
            x_dim = len(signature(func).parameters)  # Number of input arguments to the function
            width = len(self.activation_funcs)
            n_double = functions.count_double(self.activation_funcs)

            initial_weights = [torch.tensor(w).to(device) for w in weights]

            net = SymbolicNetL0(self.n_layers,
                                funcs=self.activation_funcs,
                                initial_weights=initial_weights).to(device)

            # Test the network
            with torch.no_grad():
                test_outputs = net(test_data)
                test_loss = F.mse_loss(test_outputs, test_target)
                error_test_val = test_loss.item()
                print("Test error:", error_test_val)

            return error_test_val

if __name__ == "__main__":

    parser = argparse.ArgumentParser(description="Train the EQL network.")
    parser.add_argument("--results-dir", type=str, default='results/benchmark/test_test')
    parser.add_argument("--n-layers", type=int, default=2, help="Number of hidden layers, L")
    parser.add_argument("--reg-weight", type=float, default=5e-3, help='Regularization weight, lambda')
    parser.add_argument('--learning-rate', type=float, default=1e-2, help='Base learning rate for training')
    parser.add_argument("--n-epochs1", type=int, default=10001, help="Number of epochs to train the first stage")
    parser.add_argument("--n-epochs2", type=int, default=10001,
                        help="Number of epochs to train the second stage, after freezing weights.")

    args = parser.parse_args()
    kwargs = vars(args)
    print(kwargs)

    if not os.path.exists(kwargs['results_dir']):
        os.makedirs(kwargs['results_dir'])
    meta = open(os.path.join(kwargs['results_dir'], 'args.txt'), 'a')
    import json
    meta.write(json.dumps(kwargs))
    meta.close()

    bench = Benchmark(**kwargs)

    bench.benchmark(lambda x, y, z: (x+y*z)**3, func_name="(x+yz)^3", trials=5)
    exit()
    ############### test robustness of the network ###############
    func = lambda x: x
    use_cuda = torch.cuda.is_available()
    device = torch.device("cuda" if use_cuda else "cpu")
    print("Use cuda:", use_cuda, "Device:", device)

    x_test, y_test = generate_data(func, N_TEST, range_min=DOMAIN_TEST[0], range_max=DOMAIN_TEST[1])
    y_test = y_test.squeeze()
    print(type(x_test), type(y_test))
    # Add Gaussian noise to x_test
    noisy_samples = 10
    noise = torch.normal(mean=0, std=0.1, size=(noisy_samples, *x_test.size()))
    x_test_hat = x_test.unsqueeze(0) + noise
    print("noisy", x_test_hat.shape) # shape is (10, 100, 1)
    
    models_no = [0, 1, 2, 3, 4]
    min_error = 1e10
    best_model = None
    for i in models_no:
        model, test_error_i = bench.load_and_test(x_test, y_test, f'results/benchmark_l0/test/x/trial{i}.pickle')
        if test_error_i < min_error:
            min_error = test_error_i
            best_model = model
    model = best_model
    x_test_hat = x_test_hat.to(model.output_weight.device)  # Ensure x_test_hat is on the same device as the model
    # initialize y_test_hat with shape (noisy_samples, 100, 1)
    y_test_hat = torch.zeros((noisy_samples, y_test.size(0)))
    x_test_np = x_test.cpu().numpy()
    y_test_np = y_test.cpu().numpy()
    print("x_test_np", x_test_np.shape, "y_test_np", y_test_np.shape)
    plt.plot(y_test_np[:], y_test_np[:], label="y_test")
    
    with torch.no_grad():
        for i in range(noisy_samples):
            # temp = model(x_test_hat[i,:,:])
            temp = model(x_test.to(device))
            # print("input", x_test_hat[i,:,:], "output", temp)
            # print("input", x_test, "output", temp)
            # print("temp", temp.shape)
            y_test_hat[i] = temp.squeeze()
            plt.plot(y_test_np[:], y_test_hat[i].cpu().detach().numpy()[:], label=f"y_test_hat_{i}")


    print("y_test_hat", y_test_hat.shape)
    plt.legend()
    plt.savefig("test.png")
    metric = RobustnessMetric()
    outer_dists = ["L2"]
    weights = [1]
    
    x_test_hat_np = x_test_hat.cpu().numpy()
    y_test_hat_np = y_test_hat.detach().numpy()
    # y_test_hat_np = y_test_hat_np.reshape(noisy_samples, y_test.size(0))
    print("x_test_np", x_test_np.shape, "y_test_np", y_test_np.shape, "x_test_hat_np", x_test_hat_np.shape, "y_test_hat_np", y_test_hat_np.shape)

    rm = metric.calculate_metric(x_test_np, y_test_np, x_hat=x_test_hat_np, y_hat=y_test_hat_np, outer_dist=outer_dists, weights=weights, path=f"./")

    print("Robustness metric:", rm)